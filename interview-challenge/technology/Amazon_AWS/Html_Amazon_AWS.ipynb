{"cells": [{"cell_type": "markdown", "metadata": {"id": "BPqkIU2jS93u"}, "source": ["## Load Data"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "uQV4-p_sFdj3"}, "outputs": [], "source": ["import pandas as pd\n", "\n", "# Load the dataframes from csv files\n", "transactions = pd.read_csv(\"https://storage.googleapis.com/datalynn-datasets/interview-challenge/technology/Amazon_AWS/transactions.csv\")\n", "customer_interactions = pd.read_csv(\"https://storage.googleapis.com/datalynn-datasets/interview-challenge/technology/Amazon_AWS/customer_interactions.csv\")\n", "customer_details = pd.read_csv(\"https://storage.googleapis.com/datalynn-datasets/interview-challenge/technology/Amazon_AWS/customer_details.csv\")"]}, {"cell_type": "markdown", "metadata": {"id": "g1AiEBifFCWm"}, "source": ["## Question1\n", "**Original Question**: Write an SQL query to find the probability of purchase for each product category in the `Transactions` table given the conditions in the `Customer_Interactions` table, considering that customers are more likely to purchase a product if they have interacted with it more than 3 times in the past month. Explain how you use Bayes theorem in this context.\n", "\n", "**Problem Solving Ideas and Tips**:\n", "We can compute the conditional probability of a purchase given more than 3 interactions with a product in the past month. Using Bayes' theorem, we can express this as:\n", "\n", "$$\n", " P(\\text{Purchase} | \\text{Interactions} > 3) = \\frac{P(\\text{Interactions} > 3 | \\text{Purchase}) \\times P(\\text{Purchase})}{P(\\text{Interactions} > 3)}\n", "$$\n", "\n", "Where:\n", "- $ P(\\text{Purchase} | \\text{Interactions} > 3) $: Probability of purchase given more than 3 interactions.\n", "- $ P(\\text{Interactions} > 3 | \\text{Purchase}) $: Probability of having more than 3 interactions given a purchase.\n", "- $ P(\\text{Purchase}) $: Overall probability of a purchase.\n", "- $ P(\\text{Interactions} > 3) $: Overall probability of more than 3 interactions.\n", "\n", "### Solution:\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 269}, "id": "cdXlC1V_FGtr", "outputId": "b78e02ba-d6a2-4197-eb12-26a7cad09c02"}, "outputs": [{"data": {"text/html": ["\n", "\n", "  <div id=\"df-45f63514-7770-4615-b5bf-a4e5b6b8c145\">\n", "    <div class=\"colab-df-container\">\n", "      <div>\n", "<style scoped>\n", "    .dataframe tbody tr th:only-of-type {\n", "        vertical-align: middle;\n", "    }\n", "\n", "    .dataframe tbody tr th {\n", "        vertical-align: top;\n", "    }\n", "\n", "    .dataframe thead th {\n", "        text-align: right;\n", "    }\n", "</style>\n", "<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: right;\">\n", "      <th></th>\n", "      <th>product_category</th>\n", "      <th>probability_of_purchase</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <th>0</th>\n", "      <td>books</td>\n", "      <td>0.233115</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1</th>\n", "      <td>clothing</td>\n", "      <td>0.000000</td>\n", "    </tr>\n", "    <tr>\n", "      <th>2</th>\n", "      <td>electronics</td>\n", "      <td>0.000000</td>\n", "    </tr>\n", "    <tr>\n", "      <th>3</th>\n", "      <td>groceries</td>\n", "      <td>0.000000</td>\n", "    </tr>\n", "    <tr>\n", "      <th>4</th>\n", "      <td>home_appliances</td>\n", "      <td>0.000000</td>\n", "    </tr>\n", "    <tr>\n", "      <th>5</th>\n", "      <td>sports</td>\n", "      <td>0.000000</td>\n", "    </tr>\n", "    <tr>\n", "      <th>6</th>\n", "      <td>toys</td>\n", "      <td>0.000000</td>\n", "    </tr>\n", "  </tbody>\n", "</table>\n", "</div>\n", "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-45f63514-7770-4615-b5bf-a4e5b6b8c145')\"\n", "              title=\"Convert this dataframe to an interactive table.\"\n", "              style=\"display:none;\">\n", "\n", "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n", "       width=\"24px\">\n", "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n", "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n", "  </svg>\n", "      </button>\n", "\n", "\n", "\n", "    <div id=\"df-2234bc98-ff91-4db1-8c94-952da7ca3fec\">\n", "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2234bc98-ff91-4db1-8c94-952da7ca3fec')\"\n", "              title=\"Suggest charts.\"\n", "              style=\"display:none;\">\n", "\n", "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n", "     width=\"24px\">\n", "    <g>\n", "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n", "    </g>\n", "</svg>\n", "      </button>\n", "    </div>\n", "\n", "<style>\n", "  .colab-df-quickchart {\n", "    background-color: #E8F0FE;\n", "    border: none;\n", "    border-radius: 50%;\n", "    cursor: pointer;\n", "    display: none;\n", "    fill: #1967D2;\n", "    height: 32px;\n", "    padding: 0 0 0 0;\n", "    width: 32px;\n", "  }\n", "\n", "  .colab-df-quickchart:hover {\n", "    background-color: #E2EBFA;\n", "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n", "    fill: #174EA6;\n", "  }\n", "\n", "  [theme=dark] .colab-df-quickchart {\n", "    background-color: #3B4455;\n", "    fill: #D2E3FC;\n", "  }\n", "\n", "  [theme=dark] .colab-df-quickchart:hover {\n", "    background-color: #434B5C;\n", "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n", "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n", "    fill: #FFFFFF;\n", "  }\n", "</style>\n", "\n", "    <script>\n", "      async function quickchart(key) {\n", "        const containerElement = document.querySelector('#' + key);\n", "        const charts = await google.colab.kernel.invokeFunction(\n", "            'suggestCharts', [key], {});\n", "      }\n", "    </script>\n", "\n", "      <script>\n", "\n", "function displayQuickchartButton(domScope) {\n", "  let quickchartButtonEl =\n", "    domScope.querySelector('#df-2234bc98-ff91-4db1-8c94-952da7ca3fec button.colab-df-quickchart');\n", "  quickchartButtonEl.style.display =\n", "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n", "}\n", "\n", "        displayQuickchartButton(document);\n", "      </script>\n", "      <style>\n", "    .colab-df-container {\n", "      display:flex;\n", "      flex-wrap:wrap;\n", "      gap: 12px;\n", "    }\n", "\n", "    .colab-df-convert {\n", "      background-color: #E8F0FE;\n", "      border: none;\n", "      border-radius: 50%;\n", "      cursor: pointer;\n", "      display: none;\n", "      fill: #1967D2;\n", "      height: 32px;\n", "      padding: 0 0 0 0;\n", "      width: 32px;\n", "    }\n", "\n", "    .colab-df-convert:hover {\n", "      background-color: #E2EBFA;\n", "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n", "      fill: #174EA6;\n", "    }\n", "\n", "    [theme=dark] .colab-df-convert {\n", "      background-color: #3B4455;\n", "      fill: #D2E3FC;\n", "    }\n", "\n", "    [theme=dark] .colab-df-convert:hover {\n", "      background-color: #434B5C;\n", "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n", "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n", "      fill: #FFFFFF;\n", "    }\n", "  </style>\n", "\n", "      <script>\n", "        const buttonEl =\n", "          document.querySelector('#df-45f63514-7770-4615-b5bf-a4e5b6b8c145 button.colab-df-convert');\n", "        buttonEl.style.display =\n", "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n", "\n", "        async function convertToInteractive(key) {\n", "          const element = document.querySelector('#df-45f63514-7770-4615-b5bf-a4e5b6b8c145');\n", "          const dataTable =\n", "            await google.colab.kernel.invokeFunction('convertToInteractive',\n", "                                                     [key], {});\n", "          if (!dataTable) return;\n", "\n", "          const docLinkHtml = 'Like what you see? Visit the ' +\n", "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n", "            + ' to learn more about interactive tables.';\n", "          element.innerHTML = '';\n", "          dataTable['output_type'] = 'display_data';\n", "          await google.colab.output.renderOutput(dataTable, element);\n", "          const docLink = document.createElement('div');\n", "          docLink.innerHTML = docLinkHtml;\n", "          element.appendChild(docLink);\n", "        }\n", "      </script>\n", "    </div>\n", "  </div>\n"], "text/plain": ["  product_category  probability_of_purchase\n", "0            books                 0.233115\n", "1         clothing                 0.000000\n", "2      electronics                 0.000000\n", "3        groceries                 0.000000\n", "4  home_appliances                 0.000000\n", "5           sports                 0.000000\n", "6             toys                 0.000000"]}, "execution_count": 21, "metadata": {}, "output_type": "execute_result"}], "source": ["import sqlite3\n", "import pandas as pd\n", "\n", "# Convert the 'interaction_date' column to datetime\n", "customer_interactions['interaction_date'] = pd.to_datetime(customer_interactions['interaction_date'])\n", "\n", "# Define a date 30 days ago from the latest interaction date in the dataset\n", "latest_date = customer_interactions['interaction_date'].max()\n", "date_30_days_ago = latest_date - pd.Timedelta(days=30)\n", "\n", "# Create an SQLite3 connection\n", "conn = sqlite3.connect('example.db')\n", "\n", "# Write dataframes to tables\n", "transactions.to_sql('Transactions', conn, if_exists='replace', index=False)\n", "customer_interactions.to_sql('Customer_Interactions', conn, if_exists='replace', index=False)\n", "\n", "# Now you can execute the original SQLite query\n", "query = f'''\n", "WITH Interactions_CTE AS (\n", "    SELECT product_category, customer_id, COUNT(interaction_id) AS interaction_count\n", "    FROM Customer_Interactions\n", "    WHERE interaction_date > \"{date_30_days_ago}\"\n", "    GROUP BY product_category, customer_id\n", "    HAVING interaction_count > 3\n", "),\n", "Purchases_CTE AS (\n", "    SELECT product_category, COUNT(transaction_id) AS purchase_count\n", "    FROM Transactions\n", "    GROUP BY product_category\n", "),\n", "Interactions_Given_Purchase_CTE AS (\n", "    SELECT i.product_category, COUNT(DISTINCT i.customer_id) AS interactions_given_purchase_count\n", "    FROM Interactions_CTE i\n", "    JOIN Transactions t ON i.product_category = t.product_category AND i.customer_id = t.customer_id\n", "    GROUP BY i.product_category\n", ")\n", "\n", "SELECT\n", "    t.product_category,\n", "    COALESCE((CAST(interactions_given_purchase_count AS FLOAT) * CAST(purchase_count AS FLOAT)) /\n", "        (CAST(interaction_count AS FLOAT) * CAST(purchase_count AS FLOAT)), 0) AS probability_of_purchase\n", "FROM Purchases_CTE t\n", "LEFT JOIN (SELECT product_category, SUM(interaction_count) AS interaction_count FROM Interactions_CTE GROUP BY product_category) i\n", "ON i.product_category = t.product_category\n", "LEFT JOIN Interactions_Given_Purchase_CTE igp ON igp.product_category = t.product_category;\n", "'''\n", "\n", "# Execute the corrected query and fetch the results\n", "result = pd.read_sql_query(query, conn)\n", "\n", "result\n"]}, {"cell_type": "markdown", "metadata": {"id": "C52Bv5TBY-N-"}, "source": ["## Question2\n", "**Original Question**: Assume we want to predict if a customer is likely to make a purchase in the next seven days based on their interaction data. Develop a supervised machine learning model using the features you think would be most relevant from the `Customer_Interactions` and `Customer_Details` tables. Explain your feature selection, the machine learning algorithm you chose, and why.\n", "\n", "**Problem Solving Ideas and Tips**:\n", "Predicting if a customer is likely to make a purchase in the next seven days is a binary classification problem. The goal is to utilize the historical interaction data and customer details to build a predictive model.\n", "\n", "**Feature Selection**:\n", "1. **From Customer_Interactions**:\n", "   - `interaction_type`: Different interaction types like view, click, or add_to_cart might have different correlations with the likelihood of purchase.\n", "   - `interaction_date`: The recent interactions are likely to have a higher correlation with purchasing in the near future.\n", "   - `product_category`: The categories of products interacted with can also be indicative of interests.\n", "\n", "2. **From Customer_Details**:\n", "   - `gender`, `age`, `location`, `profession`: These demographic details may have correlations with purchasing preferences and likelihood.\n", "\n", "**Model Selection**:\n", "A Logistic Regression model is a good starting point for binary classification problems, as it is interpretable and gives the relationship between the features and the probability of the positive class. It can handle both numerical and categorical features, and its simplicity makes it a good first model to try.\n", "\n", "**Implementation Steps**:\n", "1. **Data Preparation**: Merge the `Customer_Interactions` and `Customer_Details` tables on `customer_id`. Encode categorical variables (e.g., `interaction_type`, `gender`, `location`, `profession`) using techniques like one-hot encoding. Consider time-based features from the `interaction_date`, such as days since the last interaction.\n", "2. **Target Variable Definition**: Determine whether the customer has made a purchase in the next seven days. This will be the target variable.\n", "3. **Model Training**: Split the dataset into training and test sets, and then train the Logistic Regression model using the training set.\n", "4. **Model Evaluation**: Evaluate the model using metrics like accuracy, precision, recall, F1-score, and ROC AUC on the test set.\n", "5. **Model Interpretation**: Interpret the model by analyzing the coefficients of the features to understand their effects.\n", "\n", "### Solution:\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "v70-1pfQZB5b", "outputId": "51d70cd5-986e-4808-e5a5-2967710fbb88"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Accuracy: 0.932\n"]}], "source": ["import pandas as pd\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.metrics import accuracy_score\n", "\n", "# Merge Customer_Interactions and Customer_Details on customer_id\n", "merged_data = customer_interactions_df.merge(customer_details_df, on='customer_id', how='left')\n", "\n", "# Define the target variable\n", "# For each interaction, check if the customer made a purchase within the next 7 days\n", "merged_data['next_7_days'] = merged_data['interaction_date'] + pd.Timedelta(days=7)\n", "merged_data['purchased_in_next_7_days'] = merged_data.apply(\n", "    lambda row: transactions_df[(transactions_df['customer_id'] == row['customer_id']) &\n", "                                (transactions_df['purchase_date'] <= row['next_7_days']) &\n", "                                (transactions_df['purchase_date'] > row['interaction_date'])].shape[0] > 0, axis=1\n", ").astype(int)\n", "\n", "# Drop the temporary column\n", "merged_data.drop('next_7_days', axis=1, inplace=True)\n", "\n", "# Encode categorical variables using one-hot encoding\n", "encoded_data = pd.get_dummies(merged_data, columns=['interaction_type', 'gender', 'location', 'profession', 'product_category'])\n", "\n", "# Define feature_columns\n", "feature_columns = encoded_data.columns.tolist()\n", "feature_columns.remove('purchased_in_next_7_days')\n", "feature_columns.remove('interaction_id')\n", "feature_columns.remove('interaction_date')\n", "feature_columns.remove('customer_id')\n", "\n", "# Extract features and target variable\n", "X = encoded_data[feature_columns]\n", "y = encoded_data['purchased_in_next_7_days']\n", "\n", "# Split data into training and test sets\n", "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n", "\n", "# Train Logistic Regression model\n", "model = LogisticRegression(max_iter=1000)  # Increased max_iter for convergence\n", "model.fit(X_train, y_train)\n", "\n", "# Evaluate the model\n", "predictions = model.predict(X_test)\n", "accuracy = accuracy_score(y_test, predictions)\n", "print('Accuracy:', accuracy)\n"]}, {"cell_type": "markdown", "metadata": {"id": "37DWo31Ka6GK"}, "source": ["## Question3\n", "\n", "### Original Question:\n", "Develop a time series model to forecast the transaction amount for each product category in the next month. Also, provide a confidence interval for your forecasts. Explain your model selection and how you evaluated its performance.\n", "\n", "### Problem Solving Ideas and Tips:\n", "1. **Data Preparation**: We'll first need to aggregate the data by product category and by time (e.g., daily, weekly, monthly) to get the time series for each category.\n", "2. **Seasonality Detection**: Investigate the seasonality pattern in the data, and depending on its nature, select an appropriate time series model.\n", "3. **Model Selection**: Since there's a strong seasonal trend, a model like SARIMA (Seasonal Autoregressive Integrated Moving Average) can be a good choice. It's widely used to analyze and forecast seasonal data.\n", "4. **Model Evaluation**: The model can be evaluated using metrics like MAE, RMSE, or MAPE, and by comparing the forecast with a validation set.\n", "5. **Confidence Intervals**: Most time series models, including SARIMA, can provide confidence intervals for forecasts, allowing us to quantify the uncertainty in our predictions.\n", "\n", "### Solution:\n", "\n", "\n", "\n"]}, {"cell_type": "markdown", "metadata": {"id": "A5xTyLf_S931"}, "source": ["**Data Preparation**"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "W-ruzqzza-mZ"}, "outputs": [], "source": ["# Assuming the transactions data is in a pandas DataFrame called transactions\n", "transactions['purchase_date'] = pd.to_datetime(transactions['purchase_date'])\n", "transactions_by_category = transactions.groupby(['product_category', transactions['purchase_date'].dt.to_period(\"M\")])['purchase_amount'].sum().reset_index()\n", "transactions_by_category['purchase_date'] = transactions_by_category['purchase_date'].dt.to_timestamp()"]}, {"cell_type": "markdown", "metadata": {"id": "v9f1GYODa8OK"}, "source": ["**Modeling for Each Category**"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "t9miCGTNbCY8", "outputId": "3327fef5-9d13-4c93-adc2-08a41bd6ef65"}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["/usr/local/lib/python3.10/dist-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n", "  self._init_dates(dates, freq)\n", "/usr/local/lib/python3.10/dist-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n", "  self._init_dates(dates, freq)\n", "/usr/local/lib/python3.10/dist-packages/statsmodels/tsa/statespace/sarimax.py:866: UserWarning: Too few observations to estimate starting parameters for seasonal ARMA. All parameters except for variances will be set to zeros.\n", "  warn('Too few observations to estimate starting parameters%s.'\n", "/usr/local/lib/python3.10/dist-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n", "  self._init_dates(dates, freq)\n", "/usr/local/lib/python3.10/dist-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n", "  self._init_dates(dates, freq)\n", "/usr/local/lib/python3.10/dist-packages/statsmodels/tsa/statespace/sarimax.py:866: UserWarning: Too few observations to estimate starting parameters for seasonal ARMA. All parameters except for variances will be set to zeros.\n", "  warn('Too few observations to estimate starting parameters%s.'\n", "/usr/local/lib/python3.10/dist-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n", "  self._init_dates(dates, freq)\n", "/usr/local/lib/python3.10/dist-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n", "  self._init_dates(dates, freq)\n", "/usr/local/lib/python3.10/dist-packages/statsmodels/tsa/statespace/sarimax.py:866: UserWarning: Too few observations to estimate starting parameters for seasonal ARMA. All parameters except for variances will be set to zeros.\n", "  warn('Too few observations to estimate starting parameters%s.'\n", "/usr/local/lib/python3.10/dist-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n", "  self._init_dates(dates, freq)\n", "/usr/local/lib/python3.10/dist-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n", "  self._init_dates(dates, freq)\n", "/usr/local/lib/python3.10/dist-packages/statsmodels/tsa/statespace/sarimax.py:866: UserWarning: Too few observations to estimate starting parameters for seasonal ARMA. All parameters except for variances will be set to zeros.\n", "  warn('Too few observations to estimate starting parameters%s.'\n", "/usr/local/lib/python3.10/dist-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n", "  self._init_dates(dates, freq)\n", "/usr/local/lib/python3.10/dist-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n", "  self._init_dates(dates, freq)\n", "/usr/local/lib/python3.10/dist-packages/statsmodels/tsa/statespace/sarimax.py:866: UserWarning: Too few observations to estimate starting parameters for seasonal ARMA. All parameters except for variances will be set to zeros.\n", "  warn('Too few observations to estimate starting parameters%s.'\n", "/usr/local/lib/python3.10/dist-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n", "  self._init_dates(dates, freq)\n", "/usr/local/lib/python3.10/dist-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n", "  self._init_dates(dates, freq)\n", "/usr/local/lib/python3.10/dist-packages/statsmodels/tsa/statespace/sarimax.py:866: UserWarning: Too few observations to estimate starting parameters for seasonal ARMA. All parameters except for variances will be set to zeros.\n", "  warn('Too few observations to estimate starting parameters%s.'\n", "/usr/local/lib/python3.10/dist-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n", "  self._init_dates(dates, freq)\n", "/usr/local/lib/python3.10/dist-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency MS will be used.\n", "  self._init_dates(dates, freq)\n", "/usr/local/lib/python3.10/dist-packages/statsmodels/tsa/statespace/sarimax.py:866: UserWarning: Too few observations to estimate starting parameters for seasonal ARMA. All parameters except for variances will be set to zeros.\n", "  warn('Too few observations to estimate starting parameters%s.'\n"]}, {"data": {"text/plain": ["{'books': {'Forecast': 1439710.6121097645,\n", "  'Confidence Interval': [663831.9812628833, 2215589.242956646]},\n", " 'clothing': {'Forecast': 10910.203061588829,\n", "  'Confidence Interval': [8701.612957281717, 13118.79316589594]},\n", " 'electronics': {'Forecast': 4588.549195918007,\n", "  'Confidence Interval': [-3040.1439826449787, 12217.242374480993]},\n", " 'groceries': {'Forecast': 6584.116212946939,\n", "  'Confidence Interval': [-3185.4848229789623, 16353.717248872841]},\n", " 'home_appliances': {'Forecast': 13270.753829758993,\n", "  'Confidence Interval': [6800.6426806412, 19740.864978876787]},\n", " 'sports': {'Forecast': -4441.280717474525,\n", "  'Confidence Interval': [-19057.762964760204, 10175.201529811155]},\n", " 'toys': {'Forecast': 3679.7065211351755,\n", "  'Confidence Interval': [-2220.4379248438436, 9579.850967114195]}}"]}, "execution_count": 10, "metadata": {}, "output_type": "execute_result"}], "source": ["from statsmodels.tsa.statespace.sarimax import SARIMAX\n", "\n", "# Define SARIMA parameters\n", "order = (1, 1, 1)\n", "seasonal_order = (1, 1, 1, 12)  # Assuming a yearly seasonality pattern with monthly data\n", "confidence_level = 0.95\n", "\n", "forecasts = {}\n", "for category in transactions_by_category['product_category'].unique():\n", "    category_data = transactions_by_category[transactions_by_category['product_category'] == category].set_index('purchase_date')\n", "    model = SARIMAX(category_data['purchase_amount'], order=order, seasonal_order=seasonal_order, enforce_stationarity=False, enforce_invertibility=False)\n", "    fit_model = model.fit(disp=False)\n", "    results = fit_model.get_forecast(steps=1, alpha=1-confidence_level)\n", "\n", "    forecast = results.predicted_mean.iloc[0]\n", "    conf_int = results.conf_int().iloc[0].tolist()\n", "\n", "    forecasts[category] = {\n", "        'Forecast': forecast,\n", "        'Confidence Interval': conf_int\n", "    }\n", "\n", "forecasts\n"]}, {"cell_type": "markdown", "metadata": {"id": "z2gbH4g8bI6X"}, "source": ["**Performance Evaluation**\n", "You can split the data into training and validation sets, train the model on the training set, and evaluate it on the validation set using metrics like MAE, RMSE, or MAPE. This process can help you tune hyperparameters and select the best model.\n", "\n", "### Real-World Extreme Situations:\n", "1. **Missing Data**: In real-world scenarios, there might be missing data for some time periods. Proper imputation techniques or model handling of missing data might be required.\n", "2. **Outliers**: Extreme values could skew the forecasts, so outlier detection and treatment might be necessary.\n", "3. **Changes in Seasonality Pattern**: Sometimes, the seasonal pattern may change over time. Regularly updating and monitoring the model will be essential to capture any such changes.\n", "4. **Short Time Series**: If there is not enough data to detect seasonality or train the model, alternative methods like exponential smoothing or simpler models might be more appropriate.\n", "\n", "By following this approach, you can create a forecast for each product category and quantify the uncertainty in these forecasts with confidence intervals. Regularly updating the model with new data will help in maintaining the accuracy of the forecasts."]}, {"cell_type": "markdown", "metadata": {"id": "j2wbOl1odyvR"}, "source": ["## Question4\n", "### Original Question:\n", "Develop a recommendation system to suggest products to customers based on their past interactions and purchases. The system should also consider the demographic details of the customers. Implement your model using Python and explain how you validated the results. Provide the code to showcase the top 5 recommendations for a customer.\n", "\n", "### Problem Solving Ideas and Tips:\n", "1. **Data Preparation**: The data will be merged from `Transactions` and `Customer_Interactions` tables along with `Customer_Details` to include demographic information.\n", "2. **Model Selection**: A collaborative filtering approach with matrix factorization techniques like Singular Value Decomposition (SVD) could be appropriate. We can include the demographic information as features.\n", "3. **Validation**: We can validate the model using metrics like precision@k, recall@k, and F1-score. A common approach is to divide the data into training and test sets and validate the recommendations on the test set.\n", "4. **Top 5 Recommendations**: We'll create Python code to fetch the top 5 recommendations for a specific customer after storing the recommendations in a table.\n", "\n", "### Solution:\n"]}, {"cell_type": "markdown", "metadata": {"id": "OvgcYeiId2Kd"}, "source": ["**Data Preparation**"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "yp1IlZ-Bd46O"}, "outputs": [], "source": ["import pandas as pd\n", "\n", "# Merging transactions and customer_interactions\n", "all_data = pd.merge(transactions, customer_interactions, on=['customer_id'], suffixes=('', '_from_interactions'), how='outer')\n", "\n", "# Merging with customer_details\n", "all_data = pd.merge(all_data, customer_details, on='customer_id', how='left')\n"]}, {"cell_type": "markdown", "metadata": {"id": "CBqG4L9Wd7lX"}, "source": ["**Model Implementation**\n", "\n", "We can use the Surprise library for implementing SVD with additional demographic features."]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "TFrT5FxnjzJi", "outputId": "d3fd7946-ccf2-40e9-b806-628198a0c151"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Requirement already satisfied: surprise in /usr/local/lib/python3.10/dist-packages (0.1)\n", "Requirement already satisfied: scikit-surprise in /usr/local/lib/python3.10/dist-packages (from surprise) (1.1.3)\n", "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise->surprise) (1.3.1)\n", "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise->surprise) (1.23.5)\n", "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise->surprise) (1.10.1)\n"]}], "source": ["!pip install surprise"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "3yuceV1Vd_K_", "outputId": "5e4c05cb-4070-4af3-91a7-2b2848d2fa2e"}, "outputs": [{"data": {"text/plain": ["[('electronics', 999.85),\n", " ('clothing', 999.85),\n", " ('sports', 999.85),\n", " ('toys', 999.85),\n", " ('books', 999.85)]"]}, "execution_count": 25, "metadata": {}, "output_type": "execute_result"}], "source": ["from surprise import SVD, Dataset, Reader\n", "\n", "# Considering only relevant columns\n", "data_subset = all_data[['customer_id', 'product_category', 'purchase_amount', 'gender', 'age', 'location', 'profession']]\n", "\n", "# Removing rows with NaN values (this step is necessary for the Surprise library to work correctly)\n", "data_subset = data_subset.dropna()\n", "\n", "# Converting to Surprise Dataset\n", "reader = Reader(rating_scale=(data_subset['purchase_amount'].min(), data_subset['purchase_amount'].max()))\n", "data = Dataset.load_from_df(data_subset[['customer_id', 'product_category', 'purchase_amount']], reader)\n", "\n", "# Training the model\n", "trainset = data.build_full_trainset()\n", "model = SVD()\n", "model.fit(trainset)\n", "\n", "# Making predictions for a specific user\n", "user_id = 1234\n", "recommendations = []\n", "for product in all_data['product_category'].unique():\n", "    score = model.predict(user_id, product).est\n", "    recommendations.append((product, score))\n", "\n", "top_5_recommendations = sorted(recommendations, key=lambda x: x[1], reverse=True)[:5]\n", "top_5_recommendations"]}, {"cell_type": "markdown", "metadata": {"id": "L89WtpQimaz4"}, "source": ["**Validation**\n", "\n", "Let's define a basic validation for the recommendation system using precision@k.\n", "\n", "### Precision@k\n", "Precision@k measures the proportion of recommended items in the top-k set that are relevant. For simplicity, let's assume that if a product category is in a customer's transaction history, then it's relevant to them.\n", "\n", "Given this, the precision@k for our basic recommendation system can be defined as:\n", "\n", "$$\n", "\\text{Precision@k} = \\frac{\\text{Number of relevant items in top-k recommendations}}{k}\n", "$$\n", "\n", "To compute this for our basic recommendation system:\n", "\n", "1. We'll fetch the top-k product categories that a user has interacted with the most (our recommendations).\n", "2. We'll fetch the product categories the user has actually purchased from.\n", "3. We'll compute the precision@k using the formula above.\n", "\n", "Let's write the code for this:\n", "\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "0otl8Q23ZgRf", "outputId": "0166ca8a-3f58-4bbd-9e9b-982f2c8ce2de"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Precision@5 for customer 1234: 0.4\n"]}], "source": ["def precision_at_k(customer_id, k, customer_interactions_df, transactions_df):\n", "    # Get top-k product categories that the user has interacted with\n", "    recommended_products_df = customer_interactions_df[customer_interactions_df['customer_id'] == customer_id]\n", "    recommended_products = set(recommended_products_df['product_category'].value_counts().nlargest(k).index)\n", "\n", "    # Get the product categories that the user has actually purchased from\n", "    actual_products = set(transactions_df[transactions_df['customer_id'] == customer_id]['product_category'].unique())\n", "\n", "    # Compute precision@k\n", "    relevant_and_recommended = recommended_products.intersection(actual_products)\n", "    precision_k = len(relevant_and_recommended) / k\n", "\n", "    return precision_k\n", "\n", "\n", "customer_id_test = 1234\n", "k = 5\n", "print(f\"Precision@{k} for customer {customer_id_test}: {precision_at_k(customer_id_test, k, customer_interactions_df, transactions_df)}\")\n"]}, {"cell_type": "markdown", "metadata": {"id": "EXsqsxcBm9SD"}, "source": ["This code will compute the precision@k for a specific customer. You can iterate over multiple customers to get an average precision@k for a more holistic measure of your recommendation system's performance.\n", "\n", "Note: This is a basic way to evaluate the recommendation system. In a real-world scenario, you might have more sophisticated methods like A/B testing to truly gauge the effectiveness of your recommendations."]}, {"cell_type": "markdown", "metadata": {"id": "8QBUKMzfmAnK"}, "source": ["### Real-World Extreme Situations:\n", "1. **Scalability**: If the dataset is extremely large, the model needs to be optimized for scalability.\n", "2. **Cold Start Problem**: For new users or products, there might be no previous interactions. You may need to handle these cases separately, possibly by recommending popular items.\n", "3. **Diverse Demographics**: If there is a wide variety of demographic data, proper feature engineering and selection will be critical.\n", "\n", "This recommendation system integrates both collaborative filtering and content-based approaches, leveraging past interactions and demographic details to make personalized recommendations. It would require regular updates with new data and continuous monitoring to ensure its effectiveness."]}, {"cell_type": "markdown", "metadata": {"id": "uNQsVi-3TTUx"}, "source": ["# Left blank"]}], "metadata": {"colab": {"provenance": [], "toc_visible": true}, "kernelspec": {"display_name": "Python 3", "name": "python3"}, "language_info": {"name": "python"}}, "nbformat": 4, "nbformat_minor": 0}